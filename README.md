# ğŸ“§ Prompt-Driven Email Productivity Agent

An intelligent email management system powered by Google Gemini AI for categorization, summarization, and intelligent reply generation.

[![TypeScript](https://img.shields.io/badge/TypeScript-5.3-blue.svg)](https://www.typescriptlang.org/)
[![React](https://img.shields.io/badge/React-18.2-61dafb.svg)](https://reactjs.org/)
[![Node.js](https://img.shields.io/badge/Node.js-18+-green.svg)](https://nodejs.org/)

---

## âœ¨ Features

- ğŸ¤– **AI-Powered Processing**: Email categorization, summarization, action extraction, and intelligent reply generation
- ğŸ“ **Customizable Prompts**: Create and manage prompt templates for your workflow
- ğŸ’¬ **Interactive Chat Agent**: Get contextual assistance with email-related queries
- ğŸ“Œ **Smart Organization**: Pin, archive, and manage emails efficiently
- ğŸ¨ **Modern UI**: Beautiful, responsive interface with dark mode support
- ğŸ’¾ **Draft Management**: Save and edit reply drafts (never auto-sends)

---

## ğŸ› ï¸ Tech Stack

**Backend**: Node.js, Express.js, TypeScript, SQLite, Google Gemini API  
**Frontend**: React, TypeScript, Vite, Tailwind CSS

---

## ğŸ“‹ Prerequisites

- Node.js (v18+)
- npm (v9+)
- Google Gemini API Key (optional - works with mock LLM by default)

---

## ğŸš€ Quick Start

### 1. Clone and Install

```bash
git clone <repository-url>
cd OceanAI
npm run install:all
```

### 2. Configure Environment

Create `backend/.env`:

```env
PORT=3001
FRONTEND_URL=http://localhost:3000
GEMINI_API_KEY=your-api-key-here  # Optional
GEMINI_MODEL=gemini-pro            # Optional
USE_MOCK_LLM=false                 # Set to true to use mock LLM
```

### 3. Run Development Servers

**Terminal 1 - Backend:**
```bash
npm run dev:backend
```

**Terminal 2 - Frontend:**
```bash
npm run dev:frontend
```

Open `http://localhost:3000` in your browser.

---

## ğŸ“– Usage

### Email Actions
Select an email and use the action buttons:
- **ğŸ“‹ Categorize**: Classify email into categories
- **âœ… Extract Actions**: Identify actionable items
- **âœï¸ Draft Reply**: Generate context-aware reply
- **ğŸ“ Summarize**: Get concise summary
- **âš¡ Assess Priority**: Determine urgency level

### Prompt Brain
Navigate to **Prompt Brain** to create, edit, and manage custom prompt templates.

### Draft Manager
Generated replies automatically load into the Draft Manager. Edit and save drafts (they never auto-send).

### Email Agent Chat
Ask questions about selected emails using the chat interface for contextual assistance.

---

## ğŸš€ Deployment on Vercel

### Backend Deployment

1. **Create Vercel Project**
   - Import your repository
   - Set **Root Directory**: `backend`
   - Set **Build Command**: `npm install && npm run build`
   - Set **Output Directory**: `dist`
   - Set **Install Command**: `npm install`

2. **Environment Variables**
   ```
   PORT=3001
   NODE_ENV=production
   FRONTEND_URL=https://your-frontend.vercel.app
   GEMINI_API_KEY=your-api-key
   GEMINI_MODEL=gemini-pro
   USE_MOCK_LLM=false
   ```

3. **Deploy**
   - Click Deploy
   - Note your backend URL (e.g., `https://your-backend.vercel.app`)

### Frontend Deployment

1. **Create Vercel Project**
   - Import the same repository
   - Set **Root Directory**: `frontend`
   - Set **Build Command**: `npm run build`
   - Set **Output Directory**: `dist`
   - Set **Install Command**: `npm install`

2. **Environment Variables**
   ```
   VITE_API_URL=https://your-backend.vercel.app/api
   ```

3. **Deploy**
   - Click Deploy
   - Your app will be live at the Vercel URL

### Post-Deployment

- Verify backend: `https://your-backend.vercel.app/api/health`
- Check LLM status: `https://your-backend.vercel.app/api/llm/status`
- Update `FRONTEND_URL` in backend env vars to match your frontend URL exactly (no trailing slash)

---

## ğŸ“ Project Structure

```
OceanAI/
â”œâ”€â”€ backend/              # Express API server
â”‚   â”œâ”€â”€ src/
â”‚   â”‚   â”œâ”€â”€ routes/      # API endpoints
â”‚   â”‚   â”œâ”€â”€ services/    # LLM service (Gemini + mock)
â”‚   â”‚   â”œâ”€â”€ db/          # Database setup
â”‚   â”‚   â””â”€â”€ data/        # Mock data & prompts
â”‚   â””â”€â”€ data/            # SQLite database (auto-created)
â”‚
â””â”€â”€ frontend/            # React application
    â””â”€â”€ src/
        â”œâ”€â”€ components/  # React components
        â”œâ”€â”€ api.ts       # API client
        â””â”€â”€ types.ts     # TypeScript types
```

---

## ğŸ”§ Available Scripts

**Root:**
```bash
npm run dev:backend      # Start backend dev server
npm run dev:frontend     # Start frontend dev server
npm run install:all      # Install all dependencies
npm run build            # Build both services
```

**Backend:**
```bash
cd backend
npm run dev              # Dev server with hot reload
npm run build            # Compile TypeScript
npm start                # Production server
```

**Frontend:**
```bash
cd frontend
npm run dev              # Vite dev server
npm run build            # Production build
npm run preview          # Preview production build
```

---

## ğŸš€ Production Deployment (Vercel)

Both frontend and backend run on Vercel. These are the live URLs:

- Frontend: `https://prompt-driven-email-productivity-agent-c28g-h92tqnnr2.vercel.app`
- Backend (API): `https://prompt-driven-email-productivity-ag-sand.vercel.app`

### Backend (Vercel)
- **Root Directory**: `backend/`
- **Build Command**: `npm install && npm run build`
- **Output**: handled by Vercel serverless (`api/index.ts`)
- **Environment Variables**:
  - `FRONTEND_URL=https://prompt-driven-email-productivity-agent-c28g-h92tqnnr2.vercel.app`
  - `GEMINI_API_KEY=...`
  - `GEMINI_MODEL=gemini-2.0-flash` (or any available Gemini model)
  - `USE_MOCK_LLM=false`
- **Verification**: `GET /api/llm/status` returns provider/model (`gemini`, matches `GEMINI_MODEL`)

### Frontend (Vercel)
- **Root Directory**: `frontend/`
- **Build Command**: `npm run build`
- **Output Directory**: `dist`
- **Environment Variables**:
  - `VITE_API_URL=https://prompt-driven-email-productivity-ag-sand.vercel.app/api`

### Post-Deploy Checks
1. `GET /api/health` â†’ `{ status: 'ok', ... }`
2. `GET /api/llm/status` â†’ `llmProvider: "gemini"`, `model: "gemini-2.0-flash"`
3. Open frontend URL â†’ inbox loads, actions hit backend successfully.

---

## ğŸ”Œ API Endpoints

**Base URL**: `http://localhost:3001/api` (dev) or `https://your-backend.vercel.app/api` (prod)

- `GET /emails` - Get all emails
- `GET /emails/:id` - Get single email
- `POST /emails/:id/categorize` - Categorize email
- `POST /emails/:id/actions` - Extract actions
- `POST /emails/:id/reply` - Generate reply draft
- `POST /emails/:id/summarize` - Summarize email
- `POST /emails/:id/priority` - Assess priority
- `GET /prompts` - Get all prompt templates
- `GET /drafts` - Get all drafts
- `POST /chat/:emailId?` - Send chat message
- `GET /api/health` - Health check
- `GET /api/llm/status` - LLM service status

---

## ğŸ› Troubleshooting

**Backend won't start:**
- Check port 3001 is available
- Ensure `backend/data/` directory exists (auto-created)

**CORS errors:**
- Verify `FRONTEND_URL` matches frontend URL exactly (no trailing slash)
- Check environment variables are set correctly

**LLM not working:**
- Verify `GEMINI_API_KEY` is set correctly
- Check API key has available quota
- Review backend logs for errors

**Database issues:**
- Delete `backend/data/email_agent.db` to reset
- Restart backend server

---

## ğŸ”’ Security Notes

- **Drafts never auto-send** - All drafts require manual review
- Never commit `.env` files to version control
- Use environment variables for API keys in production
- Set `FRONTEND_URL` to exact domain in production (avoid wildcards)

---

## ğŸ“ License

MIT License

---

## ğŸ¤ Contributing

1. Fork the repository
2. Create a feature branch
3. Make your changes
4. Submit a pull request

---

**Made with â¤ï¸ for better email productivity**
